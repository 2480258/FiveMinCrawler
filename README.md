# 🕐Five-Minute Crawler - A Highly Customizable Crawler without Code

![example workflow](https://github.com/2480258/FiveMinCrawler/actions/workflows/gradle.yml/badge.svg)

_and can be customized in five minutes (maybe.... if you know html and other things XD)_

**⚠This program is very experimental. Stuff will be break. Use At Your Risk. Before use, please read LICENSE**

**If you have any questions, please ask it freely in [issues](https://github.com/2480258/FiveMinCrawler/issues) tab.**

[한국어 설명서(KO)](README_KO.md)

## What is this?

**Five-Minute Crawler** is basically a simple crawler, but highly customizable with JSON file.

With this feature, you can make a crawler for specific site very fast.

## Features

- Recursively downloads web pages that pass conditions specified by you (regex)
- Effectively parses downloaded HTML powered by [jsoup](https://github.com/jhy/jsoup)
- Saves parsed and downloaded things (web pages, images, and others) as you want (directory, file names and format)
- Skips downloading pages if you did before (only applies pages that you want)
- Perfectly handles referrer header
- Supports multi-threading

## Getting Started

Firstly, if you have any questions, suggestions and others then please use issue tab.

## Usage

> FiveMinCrawler.exe -u [URL] -p [ParameterPath]

### Command-line Options

    -u                         Starting URL that needed to be crawled
    -p                         Parameter file path
                               (defines custom behavior written by you)
    -r                         (Optional) Resume file (generated by this tool)
                               If you want to avoid download once you did
    -o                         (Optional) Root directory for saving file
                               Default value is where this program saved
    -v                         Use verbose log

### Expected Questions and Answers

- Q: Some of my requests are not performed.
- A: Request should not be performed if they have same URL (including redirects) or same [tag](GUIDE.md#Tag) which
  marked alias.


- Q: Some of my contents are not saved.
- A: Parsing is currently performed without JavaScript. Please verify your contents is still visible when turn
  JavaScript off.

### So, How to Customize This Crawler?

See [Customization Guide](/GUIDE.md).

### Extra Configuration

```json
{
  "MaxRequestThread": 1,
  "MaxPageLimit" : 100
}
```

| Config Name      |                     Functions                     |
|:-----------------|:-------------------------------------------------:|
| MaxRequestThread | controls how many requests processed concurrently |
| MaxPageLimit     |        limit how many page will be crawled        |

Currently, that's all.... Can be modified at fivemin.config.json

## Limitations

- Poor performance and memory usage, but will improve this. 

- No JavaScript support, but planned (with Selenium)

## Build and Test Instruction (Optional)

This project uses gradle. so just,
> gradlew

Some unit tests of this project utilize external web server.

If you want to run these tests, please refer to [this repository](https://github.com/2480258/fivemin-test-pages).




## Used Libraries

> [arrow](https://github.com/arrow-kt/arrow)
>
> [brotli](https://github.com/google/brotli)
>
> [jsoup](https://github.com/jhy/jsoup)
>
> [kotlin-logging](https://github.com/MicroUtils/kotlin-logging)
>
> [kotlinx.coroutines](https://github.com/Kotlin/kotlinx.coroutines)
>
> [kotlinx.serialization](https://github.com/Kotlin/kotlinx.serialization)
>
> [kotlinx-cli](https://github.com/Kotlin/kotlinx-cli)
>
> [mockk](https://github.com/mockk/mockk)
>
> [okhttp](https://github.com/square/okhttp)
>
> [slf4j](https://github.com/qos-ch/slf4j)
>
